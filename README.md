# Ask Your Data â€” Intelligent BI Copilot

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An intelligent Natural Language to SQL copilot that interprets queries, generates SQL, executes on DuckDB, and produces interactive visualizations with narrative insights.

## ğŸ¯ Project Goals

- **Natural Language Understanding**: Parse user questions using Llama 3.1 (Ollama)
- **Smart SQL Generation**: Convert intents to safe, parameterized SQL queries
- **Fast Analytics**: Execute on DuckDB with dbt-managed transformations
- **Interactive Visualization**: Auto-recommend Plotly charts with AI-generated narratives
- **RAG-Enhanced Context**: FAISS-powered glossary lookup for domain-specific terms

## ğŸ—ï¸ Architecture

```
User Query â†’ Intent Parser â†’ RAG Glossary â†’ SQL Generator â†’ DuckDB
                                                              â†“
                                            Streamlit UI â† Chart Recommender
```

**Tech Stack**:
- **Database**: DuckDB (in-memory analytics)
- **Transformations**: dbt-core with dbt-duckdb adapter
- **NLP**: Llama 3.1 via Ollama
- **RAG**: FAISS vector search
- **API**: FastAPI + Uvicorn
- **Frontend**: Streamlit
- **Visualization**: Plotly

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Git
- Ollama (for Llama 3.1) â€” [Install Guide](https://ollama.ai/)

### Installation

```powershell
# Clone the repository
git clone https://github.com/yourusername/ask-your-data-copilot.git
cd ask-your-data-copilot

# Create virtual environment
python -m venv ask-your-data-env
.\ask-your-data-env\Scripts\activate  # Windows
# source ask-your-data-env/bin/activate  # Unix/Mac

# Install dependencies
pip install -r requirements.txt

# Verify installation
python verify_installs.py
```

### Initial Setup

```powershell
# Download Olist dataset (Sprint 1 - Ticket 2)
# Place CSVs in data/raw/

# Run dbt transformations
cd dbt
dbt deps
dbt run
dbt test

# Build RAG glossary index
python glossary/build_index.py
```

### Running the Application

```powershell
# Start Streamlit UI
streamlit run src/ui/app.py

# In separate terminal, start FastAPI backend
uvicorn src.api.main:app --reload --port 8000
```

Visit `http://localhost:8501` for the Streamlit interface.

## ğŸ“ Project Structure

```
ask_your_data/
â”œâ”€â”€ data/                    # Raw & processed datasets
â”‚   â”œâ”€â”€ raw/                 # Olist CSVs
â”‚   â””â”€â”€ processed/           # DuckDB files
â”œâ”€â”€ dbt/                     # dbt project
â”‚   â”œâ”€â”€ models/              # SQL models
â”‚   â”‚   â”œâ”€â”€ staging/         # Raw data staging
â”‚   â”‚   â””â”€â”€ marts/           # Business logic layer
â”‚   â”œâ”€â”€ tests/               # Data quality tests
â”‚   â””â”€â”€ dbt_project.yml      # dbt configuration
â”œâ”€â”€ glossary/                # RAG glossary
â”‚   â”œâ”€â”€ business_terms.yaml  # Domain vocabulary
â”‚   â”œâ”€â”€ build_index.py       # FAISS index builder
â”‚   â””â”€â”€ index.faiss          # Vector index (generated)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/              # Data ingestion
â”‚   â”œâ”€â”€ nlp/                 # NL parsing & intents
â”‚   â”œâ”€â”€ sql/                 # SQL generation & safety
â”‚   â”œâ”€â”€ charts/              # Chart recommendations
â”‚   â”œâ”€â”€ api/                 # FastAPI routes
â”‚   â””â”€â”€ ui/                  # Streamlit app
â”œâ”€â”€ tests/                   # Unit & integration tests
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md  # AI coding guidelines
â”œâ”€â”€ Dockerfile               # Production deployment
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # This file
```

## ğŸ§© Development Sprints

**Sprint 1 â€” Foundation** (Tickets 1-4)
- âœ… Environment setup & dependency installation
- â³ Data ingestion (Olist â†’ DuckDB)
- â³ dbt transformations
- â³ RAG glossary with FAISS

**Sprint 2 â€” Core Features** (Tickets 5-8)
- â³ NL intent parsing (Llama 3.1)
- â³ SQL generation & execution
- â³ Chart recommendation + narratives
- â³ Streamlit UI integration

**Sprint 3 â€” Production** (Tickets 9-12)
- â³ Unit testing & evaluation
- â³ Performance optimization
- â³ Dockerization
- â³ Documentation & demo

## ğŸ§ª Testing

```powershell
# Run all tests
pytest tests/

# With coverage report
pytest --cov=src tests/

# Specific test module
pytest tests/test_sql_generation.py -v
```

## ğŸ“Š Dataset

**Olist Brazilian E-Commerce** (Kaggle):
- 100k orders (2016-2018)
- Customer, product, seller, payment, review data
- Geolocation information

Tables ingested: `orders`, `customers`, `products`, `sellers`, `order_items`, `payments`, `reviews`

## ğŸ” Security

- **SQL Injection Prevention**: Parameterized queries only (no string interpolation)
- **Query Validation**: Block DDL/DML operations (DROP, DELETE, UPDATE)
- **Read-Only Mode**: DuckDB connection limited to SELECT statements

## ğŸ³ Docker Deployment

```powershell
# Build image
docker build -t ask-your-data:latest .

# Run container
docker run -p 8501:8501 -p 8000:8000 ask-your-data:latest
```

## ğŸ¤ Contributing

See `.github/copilot-instructions.md` for AI-assisted development guidelines.

**Development Workflow**:
1. Activate virtual environment
2. Check out new branch for ticket
3. Follow sprint/ticket sequence
4. Add tests for new features
5. Update documentation

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- **Dataset**: Olist Brazilian E-Commerce (Kaggle)
- **LLM**: Llama 3.1 (Meta AI) via Ollama
- **Tools**: DuckDB, dbt Labs, Streamlit, FastAPI

---

**Status**: Sprint 1 in progress (Environment setup complete âœ…)

For detailed AI coding guidelines, see [.github/copilot-instructions.md](.github/copilot-instructions.md)
